{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heretofore, all of our classification algorithms dealt with data that was labelled, and all the resulting algorithms were *supervised* (Note that PCA was unsupervised, but that was an algorithm for dimensionality reduction, not classification).  Here, we develop the entry-level member of a broad class of algorithms for fitting models that are not labelled, or so-called *unsupervised* classification algorithms.  To begin with, let's look at a new dataset, called the Old Faithful dataset which relates the occurrence time to the length of eruption for the Old Faithful geyser in Yellowstone National Park:\n",
    "<img src=\"images/OldFaithful1948.jpg\">\n",
    "Since we'll be working with this dataset a little bit, it's available in the HW3 archive.  Let's load it into python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "# Note that this dataset is delimited by an arbitrary number of spaces, hence the r'\\s*' regular expression\n",
    "X = pd.read_csv('datasets/faithful.dat',delimiter=r'\\s*',skiprows=25,engine='python').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.xlabel('Eruption Duration (minutes)')\n",
    "plt.ylabel('Recurrence Interval (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the very different scales for each dimension.  K-means is highly susceptible to scale mismatch of this type because it relies on distance metrics.  We can eliminate this problem by substracting the mean and normalizing to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X = (X-X_mean)/X_std\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.xlabel('Eruption Duration')\n",
    "plt.ylabel('Recurrence Interval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better.  Now, let's run K-means on this dataset.  Let's begin by initializing the one-hot matrix of class membership.  Then, we can initialize the means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]                   # Number of data points\n",
    "n = X.shape[1]\n",
    "N = 2                            # Number of classes\n",
    "y0 = np.random.randint(0,2,m)    # Initial class membership guess\n",
    "Z = np.zeros((m,N)).astype(int)  # Create \"one hot matrix\"\n",
    "for z,j in zip(Z,y0):\n",
    "    z[j] = 1\n",
    "    \n",
    "# Initialize mu\n",
    "Mu = np.vstack([X[Z[:,0]].mean(axis=0),X[Z[:,1]].mean(axis=0)])\n",
    "print (Mu)    \n",
    "    \n",
    "# Plot initial guesses\n",
    "plt.scatter(X[:,0],X[:,1],c=np.argmax(Z,axis=1))\n",
    "plt.plot(Mu[:,0],Mu[:,1],'ro')\n",
    "\n",
    "plt.xlabel('Eruption Duration')\n",
    "plt.ylabel('Recurrence Interval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement a loop for K-means.  The first line computes the distance between every point and every mean.  The next three lines, constructs a new matrix $Z$ with a one in the column of the closest mean and a zero otherwise.  Finally, a new set of means are generated by taking the average of points assigned to that class mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the misfit\n",
    "def J(Z,Mu,X):\n",
    "    JJ = 0\n",
    "    for i in range(m):\n",
    "        for k in range(N):\n",
    "            JJ += Z[i,k]*np.sum((X[i]-Mu[k])**2)\n",
    "    return JJ\n",
    "\n",
    "# Compute distance from cluster means to points\n",
    "def distances(X,Mu):\n",
    "    D = np.zeros((m,N))\n",
    "    for i in range(m):\n",
    "        for k in range(N):\n",
    "            D[i,k] = np.sum((X[i]-Mu[k])**2)\n",
    "    return D\n",
    "\n",
    "# Generate the one-hot matrix\n",
    "def form_Z(X,Mu):\n",
    "    Z = np.zeros((m,N))\n",
    "    D = distances(X,Mu)\n",
    "    for z,d in zip(Z,D):\n",
    "        z[np.argmin(d)] = 1\n",
    "    return Z\n",
    "\n",
    "# Calculate the cluster means\n",
    "def calc_mu(X,Z):\n",
    "    Mu = np.zeros((N,n))\n",
    "    for k in range(N):\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for i in range(m):\n",
    "            numerator += Z[i,k]*X[i]\n",
    "            denominator += Z[i,k]\n",
    "        Mu[k] = numerator/denominator\n",
    "    return Mu\n",
    "        \n",
    "# Calculate the initial misfit\n",
    "Js = [J(Z,Mu,X)]\n",
    "\n",
    "# Run five iterations of k-means\n",
    "for i in range(5):\n",
    "    # Form the one-hot class membership matrix\n",
    "    Z = form_Z(X,Mu)\n",
    "    # Calculate the cluster mean\n",
    "    Mu = calc_mu(X,Z)\n",
    "    \n",
    "    # Compute the misfit (not necessary but useful for determining convergence)\n",
    "    Js.append(J(Z,Mu,X))\n",
    "      \n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=np.argmax(Z,axis=1))\n",
    "plt.plot(Mu[:,0],Mu[:,1],'ro')\n",
    "\n",
    "plt.xlabel('Eruption Duration')\n",
    "plt.ylabel('Recurrence Interval')\n",
    "plt.show()\n",
    "print (Mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the misfit values as we update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is quite efficient, and converges to the optimal value in three iterations for this problem.  However, it isn't really in line with our probabilistic approach for this class.  A good alternative is the Mixture of Gaussians model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try MOG on this same dataset.  First, we'll re-import the data, this time without normalization because we'll be fitting a multi-variate normal which can account for different scalings between the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note that this dataset is delimited by an arbitrary number of spaces, hence the r'\\s*' regular expression\n",
    "X = pd.read_csv('datasets/faithful.dat',delimiter=r'\\s*',skiprows=25,engine='python').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can initialize a few matrices.  As before, we need to initialize a few things, in this case, class means $\\mathbf{\\mu}_k$, class covariances $\\Sigma_k$, the class priors $\\pi_k$, and the responsibility matrix $\\Gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "import sys\n",
    "sys.path.insert(0,'./libraries/')\n",
    "from plot_ellipse import plot_ellipse\n",
    "\n",
    "y0 = np.random.randint(0,2,m)    # Initial class membership guess\n",
    "Z = np.zeros((m,N)).astype(int)  # Create \"one hot matrix\"\n",
    "for z,j in zip(Z,y0):\n",
    "    z[j] = 1\n",
    "    \n",
    "Gamma = Z\n",
    "Pi = np.ones(N)/N\n",
    "Sigma = np.ones((n,n,n))\n",
    "Sigma[0,:,:] = np.eye(n)\n",
    "Sigma[1,:,:] = np.eye(n)\n",
    "    \n",
    "# Initialize mu\n",
    "Mu = np.vstack([X[Z[:,0]].mean(axis=0),X[Z[:,1]].mean(axis=0)])  \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=Gamma[:,0])\n",
    "plot_ellipse(Mu[0],Sigma[0],alpha=0.3)\n",
    "plot_ellipse(Mu[1],Sigma[1],alpha=0.3)\n",
    "Mu = np.array(Mu)\n",
    "plt.plot(Mu[:,0],Mu[:,1],'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a few iterations of EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fractional class membership given parameters mu, sigma, pi (and the data)\n",
    "def calc_Gamma(Mu,Sigma,Pi,X):\n",
    "    Gamma = np.zeros((m,N))/N\n",
    "    for k in range(N):\n",
    "        pi = Pi[k]\n",
    "        mu = Mu[k]\n",
    "        sigma = Sigma[k]\n",
    "        for p in range(m):\n",
    "            Gamma[p,k] = pi*np.exp(-0.5*np.dot(X[p] - Mu[k],np.dot(np.linalg.inv(Sigma[k]),X[p]-Mu[k])))\n",
    "    Gamma /= Gamma.sum(axis=1)[:,np.newaxis]  \n",
    "    Nk = Gamma.sum(axis=0)\n",
    "    return Gamma,Nk\n",
    "       \n",
    "# Compute the parameters given fractional class membership (and the data)\n",
    "def calc_parameters(Gamma,Nk,X):\n",
    "    for k in range(N):\n",
    "        Mu[k] = 1./Nk[k]*np.sum(gamma[:,k][:,np.newaxis]*X,axis=0)\n",
    "        t = (X-Mu[k])[:,:,np.newaxis]\n",
    "        that = (gamma[:,k][:,np.newaxis]*(X-Mu[k]))[:,np.newaxis]\n",
    "        tT = np.transpose(t,(0,2,1))\n",
    "        Sigma[k]=1./Nk[k]*np.tensordot(that,tT,([0],[0])).squeeze()\n",
    "        Pi[k] = Nk[k]/m\n",
    "    return Mu,Sigma,Pi\n",
    "\n",
    "# Perform 3 iterations of EM\n",
    "for i in range(3):\n",
    "    # Compute fractional class membership\n",
    "    Gamma,Nk = calc_Gamma(Mu,Sigma,Pi,X)\n",
    "    \n",
    "    # Compute normal distribution parameters\n",
    "    Mu,Sigma,Pi = calc_parameters(Gamma,Nk,X)\n",
    "    print(Mu)\n",
    "\n",
    "    # Plot resulting distributions\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,0],X[:,1],c=Gamma[:,0])\n",
    "    plot_ellipse(Mu[0],Sigma[0],alpha=0.3)\n",
    "    plot_ellipse(Mu[1],Sigma[1],alpha=0.3)\n",
    "    Mu = np.array(Mu)\n",
    "    plt.plot(Mu[:,0],Mu[:,1],'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is very easy to cluster because it's linearly separable.  Let's try a slightly more challenging example with the iris dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from plot_ellipse import plot_ellipse\n",
    "\n",
    "data = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the iris dataset is 4-dimensional, but for the purposes of visualization and simplicity, we will use a principal components analysis (which we will discuss week after next) to reduce the dimensionality to two.  You can think of this as taking combinations of the 4 initial variables to make 2 new variables that best explain the variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(0.95,whiten=False)\n",
    "X = pca.fit_transform(data.data)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time let's use the pre-fabricated GMM implementation in scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "mix = mixture.GaussianMixture(n_components=3,max_iter=1000)\n",
    "mix.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this fitted model to predict the fractional membership matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = mix.predict(X)\n",
    "plt.scatter(X[:,0],X[:,1],c=Gamma)\n",
    "for mu,sigma in zip(mix.means_,mix.covariances_):\n",
    "    plot_ellipse(mu,sigma,alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  About the same accuracy as naive Bayes (which this algorithm is closely related to), but without the class labels.\n",
    "\n",
    "But what if we didn't know the number of classes *a priori*?  This turns out to be one of the central questions of unsupervised learning, and there are many possible answers.  One of the easiest however is to fit the model several times using different numbers of classes, and to evaluate a model selection criterion such as BIC (Bayesian Information Criterion).\n",
    "\n",
    "Bayesian Information Criterion is formally defined as\n",
    "$$\n",
    "BIC = \\ln (m) k - 2 \\ln (L),\n",
    "$$\n",
    "where $m$ is the number of data points, $k$ the number of model parameters, and $L$ the likelihood function.  The best choice of model minimizes the BIC.  Note that the BIC goes up when the number of parameters is increased, and goes down as the log likelihood increases.  Thus the BIC penalizes both data misfit and having a complex model.  Note that, in general, as the sample size increases, the likelihood goes down just because there are more observations for which misfit is added to the objective function.  This is why the $\\ln (m)$ is there: we don't want to penalize having more data, it's just there in order to scale the parameter penalty. \n",
    "\n",
    "With BIC in hand, we can fit multiple models, evaluate the BIC, and then choose the model for which BIC is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "BICs = []\n",
    "for n_clusters in range(1,7):\n",
    "    mix = mixture.GaussianMixture(n_components=n_clusters,max_iter=1000)\n",
    "    mix.fit(X)\n",
    "    BICs.append(mix.bic(X))\n",
    "\n",
    "plt.plot(range(1,7),BICs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the minimum of BIC suggests only two clusters!  Maybe this isn't so surprising.  Looking at our data again, without class labels it's not very easy to see the divide between two of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = mixture.GaussianMixture(n_components=2,max_iter=1000)\n",
    "mix.fit(X)\n",
    "\n",
    "y_pred = mix.predict(X)\n",
    "plt.scatter(X[:,0],X[:,1],c=y_pred)\n",
    "for mu,sigma in zip(mix.means_,mix.covariances_):\n",
    "    plot_ellipse(mu,sigma,alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case is a little bit challenging.  To see how BIC can help pick the number of classes, let's make up some data that is more easily separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "means = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "random_centers = np.random.randint(0,4,200)\n",
    "X = means[random_centers] + sigma*np.random.randn(200,2)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we have four distinct clusters.  Let's loop over $k$ as before, and compute the BIC for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "BICs = []\n",
    "for n_clusters in range(1,10):\n",
    "    mix = mixture.GaussianMixture(n_components=n_clusters,max_iter=1000)\n",
    "    mix.fit(X)\n",
    "    BICs.append(mix.bic(X))\n",
    "plt.plot(range(1,10),BICs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have a minimum at $k=4$, so BIC has picked out the correct number of clusters to fit.\n",
    "\n",
    "What happens if the data becomes a little bit more ambiguous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.3\n",
    "means = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "random_centers = np.random.randint(0,4,200)\n",
    "X = means[random_centers] + sigma*np.random.randn(200,2)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "BICs = []\n",
    "for n_clusters in range(1,10):\n",
    "    mix = mixture.GaussianMixture(n_components=n_clusters,max_iter=1000)\n",
    "    mix.fit(X)\n",
    "    BICs.append(mix.bic(X))\n",
    "plt.plot(range(1,10),BICs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there isn't enough division between classes to infer that there should be 4 clusters.  Instead BIC implies that we'd be better off combining two of the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "n_clusters = np.argmin(BICs)+1\n",
    "mix = mixture.GaussianMixture(n_components=n_clusters,max_iter=1000)\n",
    "mix.fit(X)\n",
    "y_pred = mix.predict(X)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y_pred)\n",
    "for mu,sigma in zip(mix.means_,mix.covariances_):\n",
    "    plot_ellipse(mu,sigma,alpha=0.3)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
