{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of classification, we have so far explored a class of model called the *generative model*, so called because it is possible to generate synthetic based on this model.  Markov models (and HMMs), graphical models, and naive Bayes all fell into this category.  This process was based on the notion that we could specify the joint probability distribution of the class and the data:\n",
    "$$P(\\mathbf{x},y),$$\n",
    "where $\\mathbf{x}$ is a vector of features, and $y$ is a class label.\n",
    "\n",
    "It was then easy to use Bayes' rule to infer the class:\n",
    "$$\n",
    "P(y|\\mathbf{x}) \\propto P(\\mathbf{x}|y)p(y).\n",
    "$$\n",
    "However, this required the specification of a statistical model for each class (what is the distribution of $\\mathbf{x}$ given y).  For example in the examples we saw for naive Bayes, we assumed that the features $\\mathbf{x}$ were normally distributed, with some mean and covariance that depended on which class they were a member of.  For example, let's recall the lobster dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division    # Eliminates annoying default integer division behavior\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('datasets/lobster_survive.dat',header=0,sep=r\"\\s{2,}\",engine='python')\n",
    "X = data['Len'].values\n",
    "\n",
    "# z-normalization\n",
    "X = X - X.mean()\n",
    "X = X/X.std()\n",
    "\n",
    "y = data['Survive'].values\n",
    "\n",
    "m = len(y)\n",
    "N = 2\n",
    "n = 1\n",
    "\n",
    "plt.hist(X[y==0],np.linspace(-3,3,10),density=True,histtype='step')\n",
    "plt.hist(X[y==1],np.linspace(-3,3,10),density=True,histtype='step')\n",
    "plt.xlabel('Lobster size (cm)')\n",
    "plt.ylabel('P(survival|lobster size)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we fit our model by finding maximum likelihood estimates, which for the mean and variance of each Gaussian turned out to be the sample mean and sample variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = X[y==0].mean()\n",
    "mu_1 = X[y==1].mean()\n",
    "\n",
    "sigma2_0 = np.cov(X[y==0])\n",
    "sigma2_1 = np.cov(X[y==1])\n",
    "\n",
    "from scipy.stats import norm\n",
    "P_X_given_y1 = norm.pdf(X,mu_0,np.sqrt(sigma2_0))\n",
    "P_X_given_y2 = norm.pdf(X,mu_1,np.sqrt(sigma2_1))\n",
    "\n",
    "plt.hist(X[y==0],np.linspace(-3,3,10),density=True,histtype='step')\n",
    "plt.hist(X[y==1],np.linspace(-3,3,10),density=True,histtype='step')\n",
    "plt.plot(X,P_X_given_y1)\n",
    "plt.plot(X,P_X_given_y2)\n",
    "\n",
    "plt.xlabel('Lobster size (cm)')\n",
    "plt.ylabel('P(survival|lobster size)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could determine the prior probability of each class (i.e. $P(y=0)=\\pi_0$) by just counting the fraction of the lobsters that survived (or the fraction that died)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_0 = sum(y==0)/len(y)\n",
    "pi_1 = sum(y==1)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification was as easy as asking which class had the higher probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the probability of survival for a lobster that is half a standard deviation below the mean in size.\n",
    "X_new = -0.5\n",
    "\n",
    "posterior_0 = norm.pdf(X_new,mu_0,np.sqrt(sigma2_0))*pi_0\n",
    "posterior_1 = norm.pdf(X_new,mu_1,np.sqrt(sigma2_1))*pi_1\n",
    "psum = posterior_0 + posterior_1\n",
    "\n",
    "posterior_0/=psum\n",
    "posterior_1/=psum\n",
    "\n",
    "print(\"Probability of lobster death is: \",posterior_0)\n",
    "print(\"Probability of lobster survival is: \",posterior_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all well and good, but sometimes it's not so easy to make an assumption about how the data are distributed!  Why should lobster size be normally distributed?  How do we even know?  This leads us to an alternative way to perform classification called *discriminative classification*, in which rather than model the features as an explicit statistical model, we'll come up with a function that takes as input the features, and outputs a probability for each class.  \n",
    "\n",
    "What function should we use for such an endeavor?  Let's begin by looking at the two-class naive Bayes model again.  By definition, \n",
    "$$\n",
    "P(y=1|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y=1)P(y=1)}{P(\\mathbf{x}|y=1)P(y=1) + P(\\mathbf{x}|y=0)P(y=0)}.\n",
    "$$\n",
    "We can factorize this as follows:\n",
    "$$\n",
    "P(y=1|\\mathbf{x}) = \\frac{1}{1 + \\frac{P(\\mathbf{x}|y=0)P(y=0)}{P(\\mathbf{x}|y=1)P(y=1)}}.\n",
    "$$\n",
    "Then making the substitution\n",
    "$$\n",
    "a(\\mathbf{x}) = \\ln \\frac{P(\\mathbf{x}|y=1)P(y=1)}{P(\\mathbf{x}|y=0)P(y=0)},\n",
    "$$\n",
    "we have that \n",
    "$$\n",
    "P(y=1|\\mathbf{x}) = \\frac{1}{1 + \\mathrm{e}^{-a}}.\n",
    "$$\n",
    "The quantity $a$ is the (logarithm of the) ratio of probabilities, also known as the odds.  Thus $a$ is called the log-odds.  This is a useful quantity because it squishes the odds down to a symmetric function on the complete real line, rather than just a positive value.  \n",
    "\n",
    "This function on the right hand side ends up being common enough that it has a name: the sigmoid (Greek for s-like)\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1 + \\mathrm{e}^{-a}}.\n",
    "$$\n",
    "The sigmoid function takes a number on the real line, and squashes it down to a value between zero and one, a proper probability.  Here's what it looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-5,5,101)\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1./(1+np.exp(-a))\n",
    "\n",
    "plt.plot(a,sigmoid(a))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some insight into what naive Bayes is doing: we're taking the log-odds of the data for the two classes, and turning it into a class probability by running it through the logistic function.  \n",
    "\n",
    "As it turns out, in this univariate case if we use the naive Bayes model, we can write $a$ as a quadratic function of the features (dropping the bold on $x$, since it's just a scalar):\n",
    "$$\n",
    "a(x) = w_2 x^2 + w_1 x + w_0 ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "w_2 = \\frac{1}{2}\\left[\\frac{1}{\\sigma^2_0} - \\frac{1}{\\sigma^2_1}\\right]\n",
    "$$\n",
    "$$\n",
    "w_1 = \\frac{\\mu_1}{\\sigma^2_1} - \\frac{\\mu_0}{\\sigma^2_0}\n",
    "$$\n",
    "$$\n",
    "w_0 = -\\frac{1}{2}\\frac{\\mu_1^2}{\\sigma_1^2} + \\frac{1}{2}\\frac{\\mu_0^2}{\\sigma_0^2} + \\ln\\frac{\\pi_1}{\\pi_0}.\n",
    "$$\n",
    "It's important to recognize that by using these definitions, we haven't done anything different from normal naive Bayes.  We can show this by computing the probability using the normal naive Bayes thing of evaluating the probability for each class, then normalizing and comparing it to what we get using these formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_2 = 0.5*(1./sigma2_0 - 1./sigma2_1)\n",
    "w_1 = mu_1/sigma2_1 - mu_0/sigma2_0\n",
    "w_0 = -0.5*mu_1**2/sigma2_1 + 0.5*mu_0**2/sigma2_0 + np.log(pi_1/pi_0)\n",
    "\n",
    "\n",
    "w_nb = np.array([w_0,w_1,w_2])\n",
    "\n",
    "posterior_0 = norm.pdf(X,mu_0,np.sqrt(sigma2_0))*pi_0\n",
    "posterior_1 = norm.pdf(X,mu_1,np.sqrt(sigma2_1))*pi_1\n",
    "psum = posterior_0 + posterior_1\n",
    "\n",
    "posterior_0/=psum\n",
    "posterior_1/=psum\n",
    "\n",
    "a = w_0 + w_1*X + w_2*X**2\n",
    "posterior_logistic = sigmoid(a)\n",
    "\n",
    "plt.plot(X,posterior_1)\n",
    "plt.plot(X,posterior_logistic,'ro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're the same, we haven't changed anything by writing the classifier in this way.  \n",
    "\n",
    "The change to logistic regression from naive Bayes comes when we *don't* use these definitions for the parameters defining the log odds $a(x)$.  Instead, what we want to do is find the values of the parameters $\\mathbf{w}$ that maximize the accuracy of the model or (equivalently) maximize the probability of a correct classification.  How do we define a function for maximizing the probability of a correct classification?  Well it should be a distribution on the boolean-valued class $y$ given the data $x$.  The Bernoulli distribution is usually appropriate for this sort of thing:\n",
    "$$\n",
    "P(y|x) = \\theta(x)^y (1-\\theta(x)^{1-y}.\n",
    "$$\n",
    "What should our mean $\\theta$ be?  It should be our prediction:\n",
    "$$\n",
    "\\theta(x) = \\sigma(w_0 + w_1 x + w_2 x^2).\n",
    "$$\n",
    "We can write this more generally as\n",
    "$$\n",
    "\\theta = \\sigma(\\mathbf{w}^T \\mathbf{\\phi}),\n",
    "$$\n",
    "where $\\mathbf{\\phi} = [1,x,x^2]$, which is basically the expansion of the features as monomials, just like we saw in linear regression.  If we consider all of our data points, we can make a matrix\n",
    "$$\n",
    "\\Phi = \\left[\\begin{array}{ccc} 1 & x_1 & x_1^2 \\\\\n",
    "                      1 & x_2 & x_2^2 \\\\\n",
    "                      \\vdots & \\vdots & \\vdots \\\\\n",
    "                      1 & x_m & x_m^2 \\end{array}\\right]\n",
    "                     $$\n",
    "                           \n",
    "If we assume that all our data points are independent, we can generate a likelihood model that we can maximize:\n",
    "$$\n",
    "P(Y|\\Phi) = \\prod_{i=1}^m P(y_i|\\phi_i),\n",
    "$$\n",
    "where $Y$ is a vector containing all of our class labels.  Substituion of the definition of the Bernoulli in, we get\n",
    "$$\n",
    "P(Y|X) = \\prod_{i=1}^m \\theta_i^{y_i} (1-\\theta_i)^{1-y_i},\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\theta_i = \\mathbf{w}^T \\phi_i.\n",
    "$$\n",
    "\n",
    "To find the best values of $\\mathbf{w}$, we can maximize this function with respect to it.  Per usual, it can be tricky to find the gradient of this function that has a product in it.  Instead, we'll find the minimum of the negative log-likelihood:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^m y_i \\ln \\theta_i + (1-y_i) \\ln (1-\\theta_i).\n",
    "$$\n",
    "This log-likelihood shows up fairly frequently, and has it's own name: cross-entropy.  You can think of it as least squares for binary variables, in the sense that it gets big when we mis-classify our data, and it becomes small when we mostly get stuff right.  To minimize it, we'll take the derivative, and try to find the place where it's zero.  The derivative of this function is pretty easy to find, and it turns out to be\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\sum_{i=1}^m [\\theta_i - y_i] \\Phi_{ij}.\n",
    "$$\n",
    "\n",
    "Unfortunately, unlike in the case of linear regression, we can't simply set these derivatives to zero and solve a linear system of equations to get the values for $w_i$.  Instead, we'll have to search for the optimal parameter values.  How should we do this?  First let's take stock of the information that we have: first, we have a formula for $\\theta$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.vander(X,N=3,increasing=True)\n",
    "\n",
    "def theta(w,Phi):\n",
    "    a = np.dot(Phi,w)\n",
    "    return sigmoid(a)\n",
    "\n",
    "# The objective function\n",
    "def L(w,Phi,y):\n",
    "    likelihood = -sum(y*np.log(theta(w,Phi)) + (1-y)*np.log(1-theta(w,Phi)))\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the derivative of this function with respect to the values of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient of the objective function\n",
    "def dLdw(w,Phi,y):\n",
    "    return np.dot(theta(w,Phi)-y,Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to just follow this gradient downhill until we can't go downhill anymore?  Absolutely!\n",
    "\n",
    "This technique is called gradient descent, and the formula is as follows:\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla_{\\mathbf{w}} \\mathcal{L}.\n",
    "$$\n",
    "Note this parameter $\\eta$; this is called the learning rate, and we'll need to adjust it to get good performance.  If it is too small, we won't adjust the parameter very fast and it may take a very long time to reach the minimum.  If it's too large we may skip over the optimal solution without ever seeing it.  There are many variants of this algorithm (we'll explore some later in the course), but for now let's just implement the simplest possible version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1.0\n",
    "eta = 5e-3\n",
    "\n",
    "cross_entropy = [L(w,Phi,y)]\n",
    "\n",
    "g_initial = np.sqrt(sum(dLdw(w,Phi,y)**2))\n",
    "rtol=1e-3\n",
    "converged=False\n",
    "while converged==False:\n",
    "    g = dLdw(w,Phi,y)\n",
    "    w -= eta*g\n",
    "\n",
    "    gnorm = np.sqrt(sum(g**2))\n",
    "    if gnorm/g_initial<rtol:\n",
    "        converged=True\n",
    "        \n",
    "    cross_entropy.append(L(w,Phi,y))\n",
    "\n",
    "plt.plot(cross_entropy)\n",
    "plt.axhline(L(w_nb,Phi,y),color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the naive Bayes solution is not optimal for this likelihood function (although it's close).  How about the actual training accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum((theta(w,Phi)>0.5) == y)/len(y))\n",
    "print(sum((posterior_1>0.5) == y)/len(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the same.  This isn't too surprising, since there isn't much flexibility in only 1D.  However, next time, we'll develop the capacity to use this model in a multi-class setting, and see how it can substantially out-perform naive Bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
